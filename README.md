# SynthV-Auto-Cover

This is a try-out to automatically cover popular vocal music.  
All algorithms are based on SynthV-studio and Stardust Infinity.  
**Warning: The current version behave poorly. You must refine your project later.**  

1. All `.js` files need to be placed in the "script" folder of Synthersizer V.  
2. `train_*.ipynb` files are based on resnet18; `train34_*.ipynb` is based on resnet34.  
3. `*_con` means this file use the second dataset only; `*_mix` means it uses both datasets.
4. To put the generated parameters into SynthV, you need to copy all the context in the `param.txt` or `pitch.txt`, then the `load_*.js` scripts are able to transport these information from the clipboard to the software. You'd better use the simplification function since there may be too many data points for SynthV to synthesize vocal music.  
5. There is a simply mixed demo covering the famous song "Invisible Wings" of Zhang Shaohan  
  
Model download: https://pan.baidu.com/s/1imqYwBmt2Of6lIh2H4YJUQ (password: o48f) 

--------------------------------------------------
# Discription of the algorithm
The infromation of the human vocal data is seperated into two classes: pitch & dynamics  
**"Pitch"** represents the base frequency of the sound, which forms the basis of music  
**"Dynamics"** represents the emotional expression, which is further divided into several categories

## pitch
For pitch information, `get_wave.m` will extract the F0 data from an `.wav` input with an `.mid` file,and it will output the data points in the `pitch.txt` in the form of "time pitchshift" ("pitchshift" is a parameter used in SynthV).  
The **$F_0$** is defined as the max point of a given function **$\frac{FFTpower(F_0)}{log_2{F_0/f_{midi}}}$** within a small range neighboring **$f_{midi}$**:    
**$FFTpower(F_0)$** represents the power of the detected frequency after appling FFT to a 200ms audio fragment.  
**$f_{midi}$** represents the ideal frequency defined in the `.mid` file.  
By using such algorithm, I can limit the **$F_0$** in a small range around the ideal frequency from `.mid`, while still catching the shifted pitch of vocal audio like appoggiatura or vibrato.

## dynamic
For dynamic information, `extract_feature.ipynb` will load a trained resnet model to get the fitted data from an `.wav` input. 
The audio input will first be clipped into fragments of ~400ms (10ms? step or overlap). Then every fragment will be transformed into a mel-spectrogram (128, 128, 1), which will be used as the input of the resnet. Five parameters (**"tension","breathiness","voicing","gender","toneshift"**) will be estimated. Several mathematical process will the applied for modification.

## datasets
There are two datasets used to train the resnet model. The first dataset is generated by `generate_wave.js`, which will randomly create seperated notes with same duration but random parameters (the 5 parameters for dynamics, pitch, and lyric). The second dataset is aimed to improve the performance of transition between notes. Therefore, `generate_wave_con.js` will create paired notes, and `clip2.m` will choose 3 different period of each pair and create the dataset. The dataset is presented in the form of individual `.mat` files.

--------------------------------------------------

The trained models will be updated later.
